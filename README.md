# ğŸš€ MLOps Platform

The **MLOps Platform** is designed to enable dynamic and efficient utilization of GPU resources in a cloud-native environment.  
Based on Kubeflow, the platform supports user-customized development environments, on-demand GPU allocation, session preservation, resource scheduling, and more.

---

## ğŸ§­ Project Overview

- **Goal**: To build a platform that allows diverse users to easily access GPU resources while minimizing resource waste and maximizing operational efficiency.
- **Key Features**
  - GPU-powered development environments using Jupyter Notebook and VS Code
  - Selection of development environment (e.g., PyTorch, TensorFlow)
  - On-demand GPU allocation and automatic deallocation
  - Session state saving and restoration
  - GPU resource scheduling and sharing
  - Operational automation and dashboard-based monitoring

---

## ğŸ›  Technology Stack

| Category               | Technologies                                                                      |
|------------------------|-----------------------------------------------------------------------------------|
| Cluster Platform       | Kubernetes, Kubeflow                                                              |
| Development Tools      | Jupyter Notebook, VS Code, Terminal-based Access                                  |
| ML Frameworks          | PyTorch, TensorFlow                                                               |
| Infrastructure         | Docker, PersistentVolumeClaim (PVC), Istio, NVIDIA GPU                            |
| Monitoring & Logging   | Prometheus, Grafana, Fluentd *(optional)*                                         |
| Authentication & Security | Dex (OIDC), Kubernetes RBAC *(optional: OPA/Gatekeeper)*                      |
| Automation & Deployment| Kustomize, Helm *(Planned: Argo Workflow)*                                       |

---

## ğŸ§© System Architecture

> ğŸ“Œ Detailed architecture documentation is in progress. Below is a high-level user flow.
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Control / Mgmt Cluster (Private DC, HA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 Git (Infra+Apps) â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Argo CD (App-of-Apps, GitOps)   Dex/IdP (OIDC SSO)   Vault+ESO   Thanos/Grafana   Kubecost       â”‚
                           â”‚  Karmada APIServer/CM/Scheduler + Webhooks (ResourceInterpreter)   Policy/OPA (opt)             â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚                               â”‚                               â”‚ mTLS
                              Karmada Control-Plane                         â”‚                               â”‚
                                           â”‚                               â”‚                               â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                 Private GPU Cluster          â”‚        â”‚                    â”‚         Public GPU Cluster   â”‚
                   â”‚ (On-Prem: V100 / TITAN D6 / A30 / 6000 SE)  â”‚        â”‚                    â”‚   (Cloud: A100/H100 etc.)   â”‚
                   â”‚                                              â”‚        â”‚                    â”‚                              â”‚
   Users â”€â”€HTTPSâ”€â”€â–¶â”‚  Ingress/Gateway + OIDC (Dex)                â”‚        â”‚                    â”‚  Ingress/Gateway + OIDC     â”‚
 (Jupyter/VSCode)  â”‚  Kubeflow UI (Notebook Server = CPU only)    â”‚        â”‚                    â”‚  (ì˜µì…˜) ê²½ëŸ‰ Kubeflow UI     â”‚
                   â”‚  K8s API + Karmada Agent                     â”‚        â”‚                    â”‚  K8s API + Karmada Agent    â”‚
                   â”‚                                              â”‚        â”‚                    â”‚                              â”‚
                   â”‚  Kueue: LocalQueue/ClusterQueue              â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ Job CRDs â”€â–¶â”‚  Kueue: LocalQueue/CQ       â”‚
                   â”‚  ResourceFlavor: gpu-bronze/silver/gold      â”‚        â”‚   (PyTorchJob/     â”‚  ResourceFlavor: gpu-gold   â”‚
                   â”‚  NVIDIA GPU Operator + DCGM Exporter         â”‚        â”‚    TFJob/Job)      â”‚  NVIDIA GPU Operator        â”‚
                   â”‚  NFD/GFD â†’ nodeLabels(gpu.tier, chip, mem)   â”‚        â”‚                    â”‚  NFD/GFD labels             â”‚
                   â”‚                                              â”‚        â”‚                    â”‚                              â”‚
                   â”‚  Storage (RWX Home) : NFS/CephFS             â”‚        â”‚                    â”‚  Storage: EBS/PD + S3/GCS   â”‚
                   â”‚  Object Store (Data/Artifacts): MinIO (S3)   â”‚â—€â”€â”€Replicate/Syncâ”€â”€â–¶ Cloud Object Storage (S3/GCS)         â”‚
                   â”‚  (venv/whl cache PVC for kernels)            â”‚        â”‚                    â”‚  (Artifacts/ckpt mirror)    â”‚
                   â”‚                                              â”‚        â”‚                    â”‚                              â”‚
                   â”‚  Service Mesh: Istio or Cilium Mesh          â”‚â—€â”€â”€â”€ mTLS / Multi-Cluster â”€â–¶â”‚  Service Mesh (federated)   â”‚
                   â”‚                                              â”‚        â”‚                    â”‚                              â”‚
                   â”‚  Serving: KServe/vLLM/Ray Serve              â”‚â—€â”€â”€ Global LB/GeoDNS â”€â”€â”€â”€â”€â”€â–¶â”‚  Serving (edge/HA/burst)    â”‚
                   â”‚                                              â”‚                             â”‚                              â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


[ì‚¬ìš© íë¦„]
1) ì‚¬ìš©ìëŠ” Privateì˜ Kubeflow í¬í„¸ì— ë¡œê·¸ì¸(SSO). Notebook ServerëŠ” CPU ì „ìš©ìœ¼ë¡œ ìƒì‹œ ê°€ë™.
2) ë…¸íŠ¸ë¶ì—ì„œ ì…€ ì‹¤í–‰ â†’ â€œGPU Jobâ€ ì œì¶œ(í…œí”Œë¦¿ì— ë“±ê¸‰/ìˆ˜ëŸ‰/ì˜ˆì‚°/ë§ˆê° ì˜µì…˜ ì£¼ì…).
3) Karmadaê°€ Jobì„ Private ìš°ì„  ì „íŒŒ(í•„ìš” ì‹œ í¼ë¸”ë¦­ë„ í›„ë³´). OverridePolicyë¡œ í´ëŸ¬ìŠ¤í„°ë³„ nodeSelector/SC ì£¼ì….
4) ê° í´ëŸ¬ìŠ¤í„°ì˜ Kueueê°€ ResourceFlavor(gpu-bronze/silver/gold)ë¡œ ì‹¤ì œ ìŠ¬ë¡¯ í• ë‹¹. ì„ê³„(ì§€ì—°/ì˜ˆì‚°/DDL) ì‹œ Publicìœ¼ë¡œ ë²„ìŠ¤íŠ¸.
5) Job PodëŠ” ì‹¤í–‰ í›„ ê²°ê³¼/ì•„í‹°íŒ©íŠ¸ë¥¼ MinIO/S3ì— ì €ì¥í•˜ê³  ì¢…ë£Œ(ì„œë²„ë¦¬ìŠ¤). Notebookì€ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°/ìˆ˜ì§‘.
6) ì„œë¹™ì€ KServe/vLLMì„ Private+Publicì— ë¶„ì‚° ë°°ì¹˜, Mesh/GeoDNSë¡œ ì§€ì—­ë³„ ë¼ìš°íŒ….
7) ê´€ì¸¡ì„±: Prometheusâ†’Thanos, Loki/Tempo, DCGM, Kubecostë¡œ ëŒ€ê¸°/ì‹¤í–‰/ë¹„ìš©/í™œìš©ë¥  ê°€ì‹œí™”.

- **User Flow**
  1. Connect via VPN
  2. Select development environment (e.g., Jupyter + PyTorch)
  3. Launch containerized environment
  4. Perform GPU-accelerated tasks
  5. Automatically return resources and save session upon exit

- **Admin Features**
  - Register and manage environment images
  - Control permissions and monitor resource usage
  - Automate user registration and access control

---

## ğŸ§­ Roadmap

**Phase 1: Core Service Setup (May, Weeks 2â€“4)**  
- Setup Kubeflow-based notebook environments *(Completed)*  
- Implement Namespace and RBAC-based user isolation *(Completed)*  
- Draft UI for development environment selection *(In progress)*  

**Phase 2: Dynamic GPU Allocation & Session Restoration (May Week 5 â€“ June Week 1)**  
- Implement GPU on-demand allocation and auto-deallocation  
- Develop session saving and restoration features  
- Integrate ML framework/version selection into UI  

**Phase 3: Resource Scheduling & Sharing (June Weeks 1â€“3)**  
- Introduce priority-based GPU scheduling policies (e.g., Kueue)  
- Apply CUDA MPS for GPU sharing among users  
- Set user-specific resource limits and concurrency control  

**Phase 4: Monitoring & Operational Automation (June Weeks 3â€“5)**  
- Build a system/resource monitoring dashboard using Grafana + Prometheus + DCGM Exporter  
- Develop anomaly detection and user alert system (e.g., Slack/Email)  
- Provide an admin-only dashboard with usage statistics and historical data  

---

## ğŸ§ª User Testing Plan

- **Test Participants**: Students affiliated with Professor Minhye Kwon  
- **Test Environment**: Linux-based Jupyter Notebook and VS Code (PyTorch image)  
- **Feedback Criteria**
  - Ease of access
  - Usability of development environments
  - Responsiveness of GPU performance
  - Session persistence and restoration

---

## ğŸ‘¥ Team & Roles

| Name            | Role               | Notes                              |
|------------------|--------------------|-------------------------------------|
| TaeUk Hwang      | PM / Lead Developer | Roadmap planning, system architecture |
| Prof. Younghan Kim | Project Advisor     | Technical and strategic guidance    |
| Test Users       | Early Feedback      | Students from Prof. Kwon's lab      |
